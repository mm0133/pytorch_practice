{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Softmax_Classification.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM4jMwK58i9p4KjrJg5ett6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"XO1wyp2UyhJ6"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","torch.manual_seed(1)\n","\n","z= torch.FloatTensor([1,2,3])\n","hypothesis=F.softmax(z,dim=0)\n","print(hypothesis)\n","\n","hypothesis.sum()\n","\n","z= torch.rand(3,5, requires_grad=True)\n","hypothesis=F.softmax(z, dim=1)\n","print(hypothesis)\n","\n","y=torch.randint(5,(3,)).long()\n","print(y)\n","\n","y_one_hot = torch.zeros_like(hypothesis)\n","y_one_hot.scatter_(1, y.unsqueeze(1),1)\n","print('x')\n","print(y_one_hot)\n","print(y.unsqueeze(1))\n","print( y.unsqueeze(0))\n","\n","cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n","cost=F.cross_entropy(z,y)\n","\n","print(F.nll_loss(F.log_softmax(z, dim=1),y))\n","print(cost)\n","\n","x_train =[[1,2,1,1],\n","          [2,1,3,2],\n","          [3,1,3,4],\n","          [4,1,5,5],\n","          [1,7,5,5],\n","          [1,2,5,6],\n","          [1,6,6,6],\n","          [1,7,7,7]]\n","\n","y_train = [2,2,2,1,1,1,0,0]\n","x_train = torch.FloatTensor(x_train)\n","y_train = torch.LongTensor(y_train) \n","print(y_train)\n","\n","W=torch.zeros((4,3), requires_grad=True)\n","b=torch.zeros(1, requires_grad=True)\n","\n","optimizer=optim.SGD([W,b], lr =0.1)\n","\n","nb_epochs=1000\n","\n","# for epoch in range(nb_epochs +1):\n","#   hypothesis = F.softmax(x_train.matmul(W)+ b, dim=1)\n","#   y_one_hot=torch.zeros_like(hypothesis)\n","#   y_one_hot.scatter_(1, y_train.unsqueeze(1),1)\n","#   cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n","  \n","#   optimizer.zero_grad()\n","#   cost.backward()\n","#   optimizer.step()\n","\n","#   if epoch% 100 == 0:\n","#     print(f'Epoch {epoch:4} Cost:{cost.item():6}')\n","\n","\n","class SoftmaxClassifierModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.linear = nn.Linear(4,3)\n","  \n","  def forward(self, x):\n","    return self.linear(x)\n","  \n","model = SoftmaxClassifierModel()\n","\n","for epoch in range(nb_epochs +1):\n","  prediction=model(x_train)\n","\n","  cost =F.cross_entropy(prediction, y_train)\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  if epoch% 100 == 0:\n","    print(f'Epoch {epoch:4} Cost:{cost.item():6}')"],"execution_count":null,"outputs":[]}]}